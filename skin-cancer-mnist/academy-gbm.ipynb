{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- ### Ensemble learning method\n",
    "\n",
    " - 여러 개의 weak learners 를 이용해 최적의 답을 찾아내는 기법\n",
    " - 대표적인 방법으로 Bagging 과 Boosting\n",
    " ![](./images/ensemble.png) \n",
    " <div align=\"right\">\n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging vs Boosting\n",
    "\n",
    "-  Bagging(Bootstrap Aggregating )\n",
    "    -  샘플을 반복적으로 추출하며(Bootstrap) ***독립적*** 으로 각 모델을 학습시켜 결과를 집계(Aggregating) 하는 방법\n",
    "    ![](./images/bagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Boosting\n",
    "     - weak learners들이 보완적 역할을 하도록 ***순차적*** 으로 학습시켜 결합시키는 방법\n",
    "\n",
    "![](./images/bb.png)\n",
    "    \n",
    "<div align=\"right\">\n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.misc import derivative\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = np.square(x)\n",
    "    return y\n",
    "\n",
    "x = np.linspace(-3,3, 50)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize= (10, 5))\n",
    "_ = ax.plot(x,f(x), 'skyblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# initial point \n",
    "initial_x = -2.5 \n",
    "# learning rate\n",
    "lr = 0.01 \n",
    "# iteration counter\n",
    "iters = 0 \n",
    "# maximum number of iterations\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cur_x = initial_x\n",
    "seq_x = []\n",
    "for i in range(max_iters):\n",
    "    # gradient descent\n",
    "    ?????\n",
    "    # sequence of x\n",
    "    if iters%10==0:\n",
    "        seq_x.append(cur_x)\n",
    "    iters = iters+1\n",
    "    print(f'Iteration:,{iters}', f'\\nx:,{cur_x}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3, 50)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize= (10, 5))\n",
    "ax.plot(x,f(x), 'skyblue')\n",
    "_ = ax.scatter(seq_x, f(seq_x), color = 'lightcoral')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
